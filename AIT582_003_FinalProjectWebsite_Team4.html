<!-- Comprehensive index.html for Metadata-Based Machine Learning Approach for Vehicle Insurance Fraud Detection -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Metadata-Based Machine Learning Approach for Vehicle Insurance Fraud Detection</title>
    <link rel="stylesheet" href="style.css">
    <style>
        /* Global Styling */
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
        }

        /* Header Styling */
        header {
            background-color: #006400; /* Green background */
            color: white; /* Title in white */
            padding: 20px;
            text-align: center;
        }

        header h1 {
            font-size: 2em;
            color: white; /* Main title in white */
            margin: 0;
        }

        /* Navigation Bar Styling */
        nav {
            background-color: #006400; /* Same Dark Green as header */
            text-align: center;
            padding: 10px 0;
        }

        nav a {
            color: black; /* Navigation links in white */
            text-decoration: none;
            margin: 0 15px;
            font-weight: bold;
            font-size: 1em;
        }

        nav a:hover {
            text-decoration: underline;
            color: #dcdcdc; /* Lighter color on hover */
        }
    </style>
</head>
<body>
    <header>
        <h1>Metadata-Based Machine Learning Approach for Vehicle Insurance Fraud Detection</h1>
        <nav>
            <a href="#introduction">Introduction</a>
            <a href="#problem">Problem Statement</a>
            <a href="#questions">Research Questions</a>
            <a href="#methodology">Methodology</a>
            <a href="#data">Data Preprocessing</a>
            <a href="#features">Feature Engineering</a>
            <a href="#models">Model Selection and Evaluation</a>
            <a href="#results">Results and Analysis</a>
            <a href="#challenges">Challenges</a>
            <a href="#future">Future Scope</a>
            <a href="#conclusion">Conclusion</a>
            <a href="#team">Team Members</a>
        </nav>
    </header>
    <main>
    <!-- Project Title and Course Information -->
    <section id="project-info" class="project-header">
        <div class="section">
            <h2>AIT 582-003: Metadata Analytics for Big Data</h2>
            <h2>Professor: Lam Phung</h3>
            <h3>George Mason University</h3>
        </div>
    </section>

        <!-- Introduction -->
        <section id="introduction">
            <div class="section">
    <h1>1. Introduction: Project Motivation and Background</h1>

    <h2>1.1 Project Motivation</h2>
    <p>Vehicle insurance fraud poses a significant challenge to the insurance industry, leading to billions of dollars in losses annually.
       Fraudulent activities include staged accidents, inflating repair costs, submitting false injury claims, and providing misleading 
       information during the insurance process. These practices not only increase operational costs but also result in higher premiums 
       for honest customers.</p>

    <p>Traditionally, insurance fraud detection has relied on manual auditing and rule-based systems. These methods are often 
       time-consuming, prone to human error, and incapable of keeping up with the evolving tactics used by fraudsters. As a result, 
       insurance companies often struggle to detect fraud promptly and accurately.</p>

    <h2>1.2 Background</h2>
    <p>With the advancement of data analytics and machine learning, automated fraud detection systems have become increasingly viable.
       Leveraging large volumes of structured and unstructured data, machine learning models can identify patterns, correlations, and 
       anomalies that might indicate fraudulent activity. This project aims to harness these advancements to build a robust vehicle 
       insurance fraud detection system.</p>

    <h2>1.3 Objectives</h2>
    <ul>
        <li>To develop a predictive model that can accurately classify insurance claims as fraudulent or non-fraudulent.</li>
        <li>To identify key metadata features that are highly indicative of fraudulent behavior.</li>
        <li>To reduce the number of false positives, thereby minimizing unnecessary investigations.</li>
        <li>To enhance operational efficiency by automating the fraud detection process.</li>
    </ul>

    <h2>1.4 Why Use Machine Learning?</h2>
    <p>Machine learning techniques have proven to be highly effective in fraud detection due to their ability to analyze complex data 
       patterns and adapt to new data. Unlike traditional rule-based approaches, machine learning models continuously improve by 
       learning from new data, making them well-suited to address the dynamic nature of insurance fraud.</p>

    <h2>1.5 Impact of the Project</h2>
    <p>Implementing a machine learning-based fraud detection system has the potential to transform the way insurance companies 
       identify and handle fraudulent claims. By automating the detection process, companies can significantly reduce manual 
       workload, improve accuracy, and maintain customer trust by minimizing false accusations.</p>

    <p>This project serves as a comprehensive approach to combating vehicle insurance fraud through data-driven methods, leveraging 
       metadata from various sources to build predictive models capable of distinguishing between legitimate and fraudulent claims.</p>
</div>
        </section>
        <!-- Problem Statement -->
        <section id="problem">
<div class="section">
    <h1>2. Problem Statement: Challenge Description</h1>
    <p>Vehicle insurance fraud is a significant challenge in the insurance industry, leading to financial losses and increased premiums 
       for honest policyholders. Fraudulent claims can include staged accidents, inflated repair costs, or false injury reports.</p>
    <p>Traditional fraud detection methods rely on manual audits and rule-based systems, which are time-consuming and prone to errors. 
       These methods fail to adapt to changing fraud techniques and are inefficient for handling large datasets.</p>
    <p>This project aims to build a machine learning-based system to detect fraudulent claims by analyzing metadata, including demographic 
       information, claim history, and vehicle details.</p>
    <h2>2.1 Research Questions</h2>
    <ol>
        <li>What are the key metadata features that indicate fraudulent claims?</li>
        <li>How do demographic factors influence fraud detection?</li>
        <li>Which machine learning models perform best in detecting fraudulent claims?</li>
        <li>What techniques can improve model accuracy given class imbalance?</li>
        <li>What evaluation metrics provide a comprehensive analysis of fraud detection performance?</li>
    </ol>

    <h2>2.2 Expected Outcomes</h2>
    <ul>
        <li>Increased accuracy and reduced false positives in fraud detection.</li>
        <li>Efficient handling of large and imbalanced datasets.</li>
        <li>Identification of key factors contributing to fraudulent claims.</li>
        <li>Improved operational efficiency through automated fraud detection.</li>
    </ul>
</div>
        </section>
	<!-- Methodology -->
        <section id="data">
<div class="section">
    <h1>3. Methodology</h1>

    <h2>3.1 Approach and Techniques</h2>
    <p>The primary goal of this project is to develop a robust machine learning model to detect fraudulent vehicle insurance claims. 
       The methodology followed consists of the following key steps:</p>

    <p><strong>Step 1: Data Collection</strong></p>
    <p>Data was collected from public insurance claim datasets, specifically designed for fraud detection. The dataset includes both 
       numeric and categorical variables related to policy information, vehicle attributes, accident details, and claim history.</p>

    <p><strong>Step 2: Data Preprocessing</strong></p>
    <p>The raw data required significant preprocessing to ensure accuracy and consistency. The following steps were applied:</p>
    <ul>
        <li>Handling Missing Values: Imputation of numerical values using the median.</li>
        <li>Encoding Categorical Data: Applied one-hot encoding and label encoding where necessary.</li>
        <li>Scaling: Standardized numerical features to improve model performance.</li>
        <li>Outlier Detection: Removed or treated outliers using the IQR method.</li>
    </ul>

    <p><strong>Step 3: Feature Engineering</strong></p>
    <p>New features were derived to enhance model accuracy:</p>
    <ul>
        <li>Address Change Frequency: Standardized categories to represent the frequency of address changes.</li>
        <li>Policy Tenure: Calculated the difference between the current year and the policy start year.</li>
        <li>Claim Filing Delay: Difference between the accident date and the date of claim filing.</li>
    </ul>

    <p><strong>Step 4: Model Selection</strong></p>
    <p>Several machine learning models were tested, including:</p>
    <ul>
        <li>Logistic Regression</li>
        <li>Decision Tree</li>
        <li>Random Forest</li>
        <li>Gradient Boosting</li>
    </ul>
    <p>Each model was evaluated based on accuracy, precision, recall, F1-score, and AUC to identify the best-performing model.</p>

    <p><strong>Step 5: Model Training and Evaluation</strong></p>
    <p>The dataset was split into training and testing sets. Models were trained and evaluated on the testing set to gauge their 
       effectiveness. The Gradient Boosting model showed the best performance with an AUC score of 0.8059.</p>

    <p><strong>Step 6: Model Validation</strong></p>
    <p>Validation techniques included cross-validation to ensure the model's robustness against overfitting. Metrics such as precision, 
       recall, and ROC-AUC were used to evaluate model performance.</p>

    <p><strong>Step 7: Deployment Strategy</strong></p>
    <p>The final model is designed for deployment in a real-world insurance claim assessment system, where it will automatically classify 
       incoming claims as fraudulent or non-fraudulent. A user-friendly interface will provide analysts with prediction insights.</p>
</div>
	</section>
       <!-- Data Preprocessing -->
        <section id="data">
<div class="section">
    <h1>4. Data Preprocessing: Data Overview, Missing Value Analysis, and Descriptive Statistics</h1>

    <h2>4.1 Data Overview</h2>
    <p>The dataset used for the Vehicle Insurance Fraud Detection project contains various features related to vehicle insurance claims.
       The data includes both numeric and categorical variables that represent policy information, vehicle details, driver characteristics,
       accident records, and claim history. The primary target variable is "FraudFound_P," which indicates whether a claim is classified as
       fraudulent (1) or non-fraudulent (0).</p>

    <p>The dataset has been sourced from public insurance claim databases and is structured as a tabular file (CSV format). 
       Key features include:</p>
    <ul>
        <li><strong>PolicyType:</strong> Type of insurance policy.</li>
        <li><strong>AccidentArea:</strong> Location category (urban or rural).</li>
        <li><strong>AddressChange_Claim:</strong> Frequency of address changes.</li>
        <li><strong>VehiclePrice:</strong> Price category of the insured vehicle.</li>
        <li><strong>DriverRating:</strong> Rating of the driver based on risk assessment.</li>
        <li><strong>FraudFound_P:</strong> Binary indicator for fraud (1: Fraud, 0: No Fraud).</li>
    </ul>

<div class="section">
    <p>The table below shows the first 5 rows of the dataset, providing an overview of the data structure and the types of features available. 
       These rows give a quick glance at some key attributes related to insurance claims.</p>

    <table border="1" style="border-collapse: collapse; width: 100%; margin: auto;">
        <tr>
            <th>Month</th>
            <th>WeekOfMonth</th>
            <th>DayOfWeek</th>
            <th>Make</th>
            <th>AccidentArea</th>
            <th>DayOfWeekClaimed</th>
            <th>MonthClaimed</th>
            <th>WeekOfMonthClaimed</th>
            <th>Sex</th>
            <th>MaritalStatus</th>
            <th>AgeOfVehicle</th>
            <th>AgeOfPolicyHolder</th>
            <th>PoliceReportFiled</th>
            <th>WitnessPresent</th>
            <th>AgentType</th>
            <th>NumberOfSuppliments</th>
            <th>AddressChange_Claim</th>
            <th>NumberOfCars</th>
            <th>Year</th>
            <th>BasePolicy</th>
        </tr>
        <tr>
            <td>Dec</td>
            <td>5</td>
            <td>Wednesday</td>
            <td>Honda</td>
            <td>Urban</td>
            <td>Tuesday</td>
            <td>Jan</td>
            <td>1</td>
            <td>Female</td>
            <td>Single</td>
            <td>3 years</td>
            <td>26 to 30</td>
            <td>No</td>
            <td>No</td>
            <td>External</td>
            <td>none</td>
            <td>1 year</td>
            <td>3 to 4</td>
            <td>1994</td>
            <td>Liability</td>
        </tr>
        <tr>
            <td>Jan</td>
            <td>3</td>
            <td>Wednesday</td>
            <td>Honda</td>
            <td>Urban</td>
            <td>Monday</td>
            <td>Jan</td>
            <td>4</td>
            <td>Male</td>
            <td>Single</td>
            <td>6 years</td>
            <td>31 to 35</td>
            <td>Yes</td>
            <td>No</td>
            <td>External</td>
            <td>none</td>
            <td>no change</td>
            <td>1 vehicle</td>
            <td>1994</td>
            <td>Collision</td>
        </tr>
        <tr>
            <td>Oct</td>
            <td>5</td>
            <td>Friday</td>
            <td>Honda</td>
            <td>Urban</td>
            <td>Thursday</td>
            <td>Nov</td>
            <td>2</td>
            <td>Male</td>
            <td>Married</td>
            <td>7 years</td>
            <td>41 to 50</td>
            <td>No</td>
            <td>No</td>
            <td>External</td>
            <td>none</td>
            <td>no change</td>
            <td>1 vehicle</td>
            <td>1994</td>
            <td>Collision</td>
        </tr>
        <tr>
            <td>Jun</td>
            <td>2</td>
            <td>Saturday</td>
            <td>Toyota</td>
            <td>Rural</td>
            <td>Friday</td>
            <td>Jul</td>
            <td>1</td>
            <td>Male</td>
            <td>Married</td>
            <td>more than 7</td>
            <td>51 to 65</td>
            <td>Yes</td>
            <td>No</td>
            <td>External</td>
            <td>more than 5</td>
            <td>no change</td>
            <td>1 vehicle</td>
            <td>1994</td>
            <td>Liability</td>
        </tr>
        <tr>
            <td>Jan</td>
            <td>5</td>
            <td>Monday</td>
            <td>Honda</td>
            <td>Urban</td>
            <td>Tuesday</td>
            <td>Feb</td>
            <td>2</td>
            <td>Female</td>
            <td>Single</td>
            <td>5 years</td>
            <td>31 to 35</td>
            <td>No</td>
            <td>No</td>
            <td>External</td>
            <td>none</td>
            <td>no change</td>
            <td>1 vehicle</td>
            <td>1994</td>
            <td>Collision</td>
        </tr>
    </table>
</div>

    <h2>4.2 Missing Value Analysis</h2>
<p>The dataset used for the Vehicle Insurance Fraud Detection project contains a total of 15,420 rows and 33 columns.</p>

<p>The data types of the columns include:</p>
<ul>
    <li>Object (17 columns)</li>
    <li>Integer (10 columns)</li>
    <li>Float (6 columns)</li>
</ul>

<p>After inspecting the dataset, it was found that some columns contained a significant number of missing values. 
The missing value analysis revealed the following:</p>

<h3>Missing Values per Column:</h3>
<ul>
    <li>Days_Policy_Accident: 15,420 (100%)</li>
    <li>Days_Policy_Claim: 15,420 (100%)</li>
    <li>AgeOfVehicle: 15,420 (100%)</li>
    <li>AgeOfPolicyHolder: 15,420 (100%)</li>
    <li>NumberOfSuppliments: 15,420 (100%)</li>
    <li>NumberOfCars: 15,420 (100%)</li>
</ul>

<h3>Total Missing Values:</h3>
<p>The total number of missing values across all columns is <strong>92,520</strong>, which accounts for approximately 
18.2% of the entire dataset.</p>

<h3>Handling Missing Values:</h3>
<ul>
    <li><strong>Drop Columns:</strong> Columns with 100% missing values were dropped since they do not contribute any information.</li>
    <li><strong>Imputation:</strong> For other columns with fewer missing values, imputation techniques such as mean, median, or mode were applied.</li>
</ul>

<h3>Visualization of Missing Data:</h3>
<p>The pie chart below shows the proportion of missing and non-missing data in the dataset:</p>
<img src="assets/Missingvalue_analysis.png" alt="Missing Value Analysis" class="small-size"/>

<p>As seen from the pie chart, about 18.2% of the data is missing, which significantly affects the data quality. 
Handling these missing values is crucial to building an effective fraud detection model.</p>

    <h2>4.3 Descriptive Statistics</h2>
    <p>Descriptive statistics help summarize the key characteristics of the dataset. In this project, we analyzed the following:</p>
    <ul>
        <li><strong>Mean:</strong> Average value for each numerical attribute.</li>
        <li><strong>Median:</strong> Central value that divides the data into two equal halves.</li>
        <li><strong>Standard Deviation:</strong> Measure of data variability.</li>
        <li><strong>Minimum and Maximum:</strong> Range of values in each column.</li>
    </ul>

    <h3>Summary Statistics Table:</h3>
    <div class="section">
    <p>The table below provides the summary statistics for key numerical features in the dataset. 
       It includes metrics such as count, mean, standard deviation (std), minimum (min), 25th percentile (25%), 
       median (50%), 75th percentile (75%), and maximum (max).</p>

    <table border="1" style="border-collapse: collapse; width: 80%; margin: auto;">
        <tr>
            <th>Feature</th>
            <th>Count</th>
            <th>Mean</th>
            <th>Std</th>
            <th>Min</th>
            <th>25%</th>
            <th>50%</th>
            <th>75%</th>
            <th>Max</th>
        </tr>
        <tr>
            <td>WeekOfMonth</td>
            <td>15420</td>
            <td>2.79</td>
            <td>1.29</td>
            <td>1.0</td>
            <td>2.0</td>
            <td>3.0</td>
            <td>4.0</td>
            <td>5.0</td>
        </tr>
        <tr>
            <td>WeekOfMonthClaimed</td>
            <td>15420</td>
            <td>2.69</td>
            <td>1.26</td>
            <td>1.0</td>
            <td>2.0</td>
            <td>3.0</td>
            <td>4.0</td>
            <td>5.0</td>
        </tr>
        <tr>
            <td>Age</td>
            <td>15420</td>
            <td>39.86</td>
            <td>13.49</td>
            <td>0.0</td>
            <td>31.0</td>
            <td>38.0</td>
            <td>48.0</td>
            <td>80.0</td>
        </tr>
        <tr>
            <td>FraudFound_P</td>
            <td>15420</td>
            <td>0.06</td>
            <td>0.24</td>
            <td>0.0</td>
            <td>0.0</td>
            <td>0.0</td>
            <td>0.0</td>
            <td>1.0</td>
        </tr>
        <tr>
            <td>PolicyNumber</td>
            <td>15420</td>
            <td>7710.50</td>
            <td>4451.51</td>
            <td>1.0</td>
            <td>3855.75</td>
            <td>7710.5</td>
            <td>11565.25</td>
            <td>15420.0</td>
        </tr>
        <tr>
            <td>RepNumber</td>
            <td>15420</td>
            <td>8.48</td>
            <td>4.60</td>
            <td>1.0</td>
            <td>5.0</td>
            <td>8.0</td>
            <td>12.0</td>
            <td>16.0</td>
        </tr>
        <tr>
            <td>Deductible</td>
            <td>15420</td>
            <td>407.70</td>
            <td>43.95</td>
            <td>300.0</td>
            <td>400.0</td>
            <td>400.0</td>
            <td>400.0</td>
            <td>700.0</td>
        </tr>
        <tr>
            <td>DriverRating</td>
            <td>15420</td>
            <td>2.49</td>
            <td>1.12</td>
            <td>1.0</td>
            <td>1.0</td>
            <td>2.0</td>
            <td>3.0</td>
            <td>4.0</td>
        </tr>
        <tr>
            <td>Year</td>
            <td>15420</td>
            <td>1994.87</td>
            <td>0.80</td>
            <td>1994.0</td>
            <td>1994.0</td>
            <td>1995.0</td>
            <td>1996.0</td>
            <td>1996.0</td>
        </tr>
    </table>
</div>


    <h3>Descriptive Statistics Visualization:</h3>
    <img src="assets/Descriptive_statistics.png" alt="Descriptive Statistics"/>

    <h2>4.4 Insights from Data Preprocessing</h2>
    <p>The data preprocessing phase played a crucial role in preparing the dataset for model training. The key takeaways include:</p>
    <ul>
        <li>Proper handling of missing values increased data consistency and reduced the risk of biased predictions.</li>
        <li>Standardizing and normalizing data helped maintain uniformity across numerical variables.</li>
        <li>Encoding categorical variables ensured that machine learning models could process the data effectively.</li>
    </ul>

    <p>The processed data, now free from missing values and properly encoded, is ready for the next phase of model development and evaluation.</p>
</div>

        </section>
        <!-- Feature Engineering -->
        <section id="features">
<div class="section">
    <h1>5. Feature Engineering</h1>

    <h2>5.1 Objective</h2>
    <p>The goal of feature engineering in this project is to create new attributes from existing data to improve the model's ability to 
       distinguish between fraudulent and non-fraudulent claims. Carefully designed features can capture hidden patterns and 
       relationships that basic data might not reveal directly.</p>

    <h2>5.2 Key Features Engineered</h2>
    <p>After analyzing the data and understanding the domain, the following new features were derived:</p>

    <p><strong>1. Address Change Frequency</strong></p>
    <p>This feature categorizes how frequently the policyholder changed their address in the past. Frequent changes might indicate 
       potential fraud, as fraudsters often use fake addresses.</p>
    <ul>
        <li><strong>No Change:</strong> No address change within the policy period.</li>
        <li><strong>1 Year:</strong> Changed address within one year.</li>
        <li><strong>2-3 Years:</strong> Changed address in the last two to three years.</li>
        <li><strong>4+ Years:</strong> Changed address more than four years ago.</li>
    </ul>

    <p><strong>2. Policy Tenure</strong></p>
    <p>This feature calculates the difference between the current year and the year the policy was initiated. Longer tenure usually 
       indicates a stable and less risky policyholder.</p>
    <p><strong>Formula:</strong> Policy Tenure = Current Year - Policy Start Year</p>
    <h3>Visualization:</h3>
    <img src="assets/Policy_tenure.png" alt="Policy Tenure Distribution" />

    <h2>5.3 Importance of Feature Engineering</h2>
    <p>Feature engineering significantly enhances the model's predictive power by incorporating domain knowledge into the data. 
       By crafting meaningful features, we can reveal hidden patterns and relationships, leading to more accurate fraud detection.</p>

    <h2>5.4 Insights from Engineered Features</h2>
    <ul>
        <li>Policyholders with frequent address changes are more likely to file fraudulent claims.</li>
        <li>Shorter policy tenure correlates with a higher risk of fraud.</li>
        <li>Delayed claim filings are more common among fraudulent cases.</li>
    </ul>

    <p>The new features created through feature engineering helped the model capture complex relationships between variables and 
       improved the overall classification accuracy.</p>
</div>
        </section>
	
	<!--Model Selection and Evaluation Section -->
	<section id="results">
<div class="section">
    <h1>6. Model Selection and Evaluation</h1>

    <h2>6.1 Model Selection</h2>
    <p>The primary objective of model selection is to identify the best algorithm for classifying vehicle insurance claims as fraudulent 
       or non-fraudulent. We tested several machine learning models to determine the most accurate and efficient approach.</p>

    <p><strong>Models Tested:</strong></p>
    <ul>
        <li>Logistic Regression</li>
        <li>Decision Tree</li>
        <li>Random Forest</li>
        <li>Gradient Boosting</li>
    </ul>

    <h2>6.2 Evaluation Metrics</h2>
    <p>To accurately assess the performance of each model, we used the following evaluation metrics:</p>
    <ul>
        <li><strong>Accuracy:</strong> Measures the proportion of correctly classified instances.</li>
        <li><strong>Precision:</strong> Proportion of correctly predicted fraudulent claims among all predicted fraud cases.</li>
        <li><strong>Recall:</strong> Proportion of correctly identified fraudulent claims among all actual fraud cases.</li>
        <li><strong>F1-Score:</strong> Harmonic mean of precision and recall, balancing both metrics.</li>
        <li><strong>AUC-ROC:</strong> Area under the ROC curve, indicating the model’s ability to distinguish between classes.</li>
    </ul>

    <h2>6.3 Model Performance</h2>
    <p>After evaluating all the models, Gradient Boosting showed the highest accuracy and AUC score among the tested algorithms. 
       The key performance metrics are summarized as follows:</p>

    <h3>Model Performance Summary:</h3>
    <table>
        <tr>
            <th>Model</th>
            <th>Accuracy</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>F1-Score</th>
            <th>AUC</th>
        </tr>
        <tr>
            <td>Logistic Regression</td>
            <td>64.89%</td>
            <td>13.22%</td>
            <td>87.05%</td>
            <td>22.96%</td>
            <td>0.7916</td>
        </tr>
        <tr>
            <td>Decision Tree</td>
            <td>66.00%</td>
            <td>14.33%</td>
            <td>80.45%</td>
            <td>24.23%</td>
            <td>0.6600</td>
        </tr>
        <tr>
            <td>Random Forest</td>
            <td>93.99%</td>
            <td>0.00%</td>
            <td>0.00%</td>
            <td>0.00%</td>
            <td>0.7943</td>
        </tr>
        <tr>
            <td>Gradient Boosting</td>
            <td>93.82%</td>
            <td>16.67%</td>
            <td>0.72%</td>
            <td>1.43%</td>
            <td>0.8059</td>
        </tr>
    </table>

    <h2>6.4 Insights from Model Evaluation</h2>
    <ul>
        <li>Gradient Boosting outperformed other models, showing the best trade-off between precision and recall.</li>
        <li>Logistic Regression demonstrated good recall but low precision, indicating many false positives.</li>
        <li>Random Forest achieved high accuracy but failed to detect fraudulent claims effectively.</li>
        <li>Decision Tree had a moderate balance but lacked the robustness seen in ensemble methods.</li>
    </ul>

    <p>Based on the evaluation, Gradient Boosting was selected as the final model for deployment due to its superior AUC and balanced performance.</p>
</div>

        </section>

        <!-- Results and Analysis -->
        <section id="results">
<div class="section">
    <h1>7. Results and Analysis</h1>

    <h2>7.1 Accuracy Comparison</h2>
    <p>After evaluating multiple models, the Gradient Boosting model demonstrated the highest accuracy among the tested algorithms. 
       The following graph shows the accuracy comparison between different models:</p>
    <img src="assets/Accuracy_comparisions.png" alt="Description of Image">

    <h2>7.2 ROC Curve Analysis</h2>
    <p>The ROC curve is a graphical representation that demonstrates the trade-off between the true positive rate (TPR) and false positive rate (FPR) 
       for each model. The area under the curve (AUC) indicates the model's ability to distinguish between fraudulent and non-fraudulent claims. 
       Gradient Boosting achieved the highest AUC of 0.8059, indicating superior performance compared to other models.</p>
    <img src="assets/ROC.png" alt="ROC Curve Comparison" />

    <h2>7.3 Confusion Matrix</h2>
    <p>The confusion matrix provides a detailed breakdown of correct and incorrect predictions. It shows the true positives (TP), true negatives (TN), 
       false positives (FP), and false negatives (FN) for each model. The Gradient Boosting model, despite achieving high accuracy, displayed a 
       relatively low recall for detecting fraudulent claims, highlighting the challenge of class imbalance.</p>
    <img src="assets/Confusion_matrix.png" alt="Confusion matrix">

    <h2>7.4 Key Insights</h2>
    <ul>
        <li>The Gradient Boosting model displayed the best overall performance, with the highest AUC of 0.8059.</li>
        <li>Although Random Forest achieved high accuracy, it failed to correctly identify fraudulent cases due to class imbalance.</li>
        <li>Logistic Regression showed good recall but suffered from low precision, resulting in a high number of false positives.</li>
        <li>The Decision Tree model showed balanced accuracy but lacked robustness compared to Gradient Boosting.</li>
    </ul>

    <h2>7.5 Challenges in Fraud Detection</h2>
    <p>The primary challenge faced during model evaluation was class imbalance. The number of non-fraudulent claims significantly exceeded 
       the fraudulent ones, leading to models biased towards predicting non-fraud cases. Techniques such as SMOTE (Synthetic Minority 
       Over-sampling Technique) were considered to address this imbalance, but further analysis is required to balance recall and precision effectively.</p>

    <h2>7.6 Conclusion</h2>
    <p>Based on the evaluation metrics and analysis, the Gradient Boosting model was selected as the most effective approach for detecting 
       fraudulent insurance claims. While its recall remains relatively low, the model's high AUC indicates a good balance between 
       sensitivity and specificity. Future improvements should focus on addressing class imbalance to enhance recall without sacrificing precision.</p>
</div>
        </section>
	<!-- Research Questions and Solutions -->
	<section id="problems and solutions">
<div class="section">
    <h1>8. Research Questions and Solutions</h1>

    <h2>8.1 What are the key metadata features that indicate fraudulent claims?</h2>
    <p><strong>Solution Approach:</strong> We performed feature engineering to derive new attributes from existing data. Key features like 
       <em>Address Change Frequency</em>, <em>Policy Tenure</em>, and <em>Claim Filing Delay</em> were identified as critical in predicting fraud.</p>
    <p><strong>Visualization:</strong> The feature importance graph generated using Gradient Boosting shows the significance of these engineered features:</p>
    <img src="assets/Feature_importance.png" alt="Feature Importance for Fraud Detection" />
    <p><strong>Explanation:</strong> The graph indicates that <em>Address Change Frequency</em> and <em>Policy Tenure</em> are the most influential 
       factors in predicting fraud. Frequent address changes and shorter policy tenures are strong indicators of potentially fraudulent claims.</p>

    <h2>8.2 How do demographic factors influence fraud detection?</h2>
    <p><strong>Solution Approach:</strong> Demographic attributes such as age, gender, and driver rating were analyzed to understand their correlation with fraud.</p>
    <p><strong>Visualization:</strong> The following graph shows the distribution of fraud based on demographic attributes:</p>
    <img src="assets/demographic.png" alt="Demographic Influence on Fraud" />
    <p><strong>Explanation:</strong> The analysis revealed that younger drivers and those with a lower driver rating are more likely to be involved in fraudulent claims. 
       Gender did not show a significant difference in fraud occurrence.</p>

    <h2>8.3 Which machine learning models perform best in detecting fraudulent claims?</h2>
    <p><strong>Solution Approach:</strong> We compared four machine learning models: Logistic Regression, Decision Tree, Random Forest, and Gradient Boosting. 
       Each model was evaluated based on accuracy, precision, recall, F1-score, and AUC.</p>
    <p><strong>Visualization:</strong> The following ROC curve shows the model performance comparison:</p>
    <img src="assets/ROC.png" alt="Model ROC Curve Comparison" />
    <p><strong>Explanation:</strong> The Gradient Boosting model outperformed others, achieving the highest AUC of 0.8059. Logistic Regression had high recall but low precision, 
       while Random Forest, despite high accuracy, failed to detect fraud effectively due to class imbalance.</p>

    <h2>8.4 What techniques can improve model accuracy given class imbalance?</h2>
    <p><strong>Solution Approach:</strong> The dataset showed a significant class imbalance, with non-fraudulent claims vastly outnumbering fraudulent ones. 
       To address this, we used techniques like <em>SMOTE (Synthetic Minority Over-sampling Technique)</em> and <em>class weighting</em>.</p>
    <p><strong>Visualization:</strong> The distribution of classes before and after applying SMOTE is shown below:</p>
    <img src="assets/SMOTE_effect.png" alt="SMOTE Class Distribution" />
    <p><strong>Explanation:</strong> After applying SMOTE, the number of fraudulent and non-fraudulent samples became more balanced, leading to improved recall without 
       significantly compromising precision.</p>

    <h2>8.5 What evaluation metrics provide a comprehensive analysis of fraud detection performance?</h2>
    <p><strong>Solution Approach:</strong> We selected accuracy, precision, recall, F1-score, and AUC as the key metrics to evaluate the model's effectiveness. 
       These metrics collectively provide a balanced view of the model's ability to correctly identify fraudulent cases while minimizing false positives.</p>
    <p><strong>Visualization:</strong> The performance metrics table for all tested models:</p>
    <table>
        <tr>
            <th>Model</th>
            <th>Accuracy</th>
            <th>Precision</th>
            <th>Recall</th>
            <th>F1-Score</th>
            <th>AUC</th>
        </tr>
        <tr>
            <td>Logistic Regression</td>
            <td>64.89%</td>
            <td>13.22%</td>
            <td>87.05%</td>
            <td>22.96%</td>
            <td>0.7916</td>
        </tr>
        <tr>
            <td>Decision Tree</td>
            <td>66.00%</td>
            <td>14.33%</td>
            <td>80.45%</td>
            <td>24.23%</td>
            <td>0.6600</td>
        </tr>
        <tr>
            <td>Random Forest</td>
            <td>93.99%</td>
            <td>0.00%</td>
            <td>0.00%</td>
            <td>0.00%</td>
            <td>0.7943</td>
        </tr>
        <tr>
            <td>Gradient Boosting</td>
            <td>93.82%</td>
            <td>16.67%</td>
            <td>0.72%</td>
            <td>1.43%</td>
            <td>0.8059</td>
        </tr>
    </table>
    <p><strong>Explanation:</strong> AUC is the most informative metric in the context of fraud detection, as it measures the ability of the model 
       to differentiate between positive (fraud) and negative (non-fraud) cases. Gradient Boosting, with the highest AUC, was chosen for final deployment.</p>
</div>
	</section>
	<!-- Challenges -->
        <section id="challenges">
<div class="section">
    <h1>9. Challenges: Class Imbalance and Model Limitations</h1>

    <h2>9.1 Class Imbalance Issue</h2>
    <p>One of the major challenges faced during this project was the significant class imbalance in the dataset. Fraudulent insurance claims 
       were vastly outnumbered by non-fraudulent claims, making it difficult for machine learning models to correctly identify fraudulent cases.</p>

    <p><strong>Impact of Class Imbalance:</strong></p>
    <ul>
        <li>Models tend to be biased towards the majority class, predicting non-fraudulent claims more frequently.</li>
        <li>High accuracy is misleading when the model fails to correctly detect fraudulent cases.</li>
        <li>Models with high accuracy may have poor recall, indicating many false negatives.</li>
    </ul>

    <h2>9.2 Addressing Class Imbalance</h2>
    <p>To mitigate the issue of class imbalance, the following techniques were applied:</p>
    <ul>
        <li><strong>SMOTE (Synthetic Minority Over-sampling Technique):</strong> Generated synthetic samples to balance the number of fraudulent and non-fraudulent claims.</li>
        <li><strong>Class Weighting:</strong> Adjusted the model to give higher importance to minority class instances, encouraging the model to pay more attention to fraudulent claims.</li>
        <li><strong>Resampling:</strong> Combined under-sampling of the majority class and over-sampling of the minority class to create a more balanced training dataset.</li>
    </ul>

    <h2>9.3 Model Limitations</h2>
    <p>While the Gradient Boosting model achieved high accuracy, it had certain limitations when applied to the highly imbalanced dataset.</p>
    <p><strong>Key Limitations:</strong></p>
    <ul>
        <li><strong>Low Recall:</strong> Despite high accuracy, the model's recall was significantly low, indicating a failure to capture fraudulent claims effectively.</li>
        <li><strong>Precision-Recall Trade-off:</strong> Efforts to increase recall often led to a decrease in precision, resulting in more false positives.</li>
        <li><strong>Overfitting Risk:</strong> Complex ensemble methods like Gradient Boosting risk overfitting when dealing with noisy data.</li>
    </ul>

    <h2>9.4 Overcoming Model Limitations</h2>
    <p>To enhance the model’s ability to detect fraud without significantly increasing false positives, we employed the following strategies:</p>
    <ul>
        <li><strong>Hyperparameter Tuning:</strong> Adjusted parameters like learning rate, maximum depth, and number of estimators to optimize model performance.</li>
        <li><strong>Cross-Validation:</strong> Used k-fold cross-validation to ensure robustness and reduce overfitting.</li>
        <li><strong>Ensemble Techniques:</strong> Combining Gradient Boosting with other models, like Random Forest, to balance accuracy and recall.</li>
    </ul>

    <h2>9.5 Insights and Future Directions</h2>
    <p>Despite implementing SMOTE and class weighting, the fundamental challenge of identifying rare fraudulent claims persisted. 
       The current model shows good generalization for non-fraud cases but struggles with rare positive cases. Future improvements could include:</p>
    <ul>
        <li>Exploring advanced ensemble methods like XGBoost with balanced class weights.</li>
        <li>Utilizing anomaly detection techniques to capture rare fraud cases more effectively.</li>
        <li>Incorporating domain-specific features that could better distinguish fraudulent behavior.</li>
    </ul>
</div>

        </section>
	<!-- Futurework -->
        <section id="futurework">
<div class="section">
    <h1>11. Future Scope</h1>
    <p>The future scope of this project primarily revolves around addressing the challenges associated with class imbalance and enhancing model accuracy. 
       One of the key areas of improvement is to implement advanced techniques like <strong>SMOTE (Synthetic Minority Over-sampling Technique)</strong> 
       and <strong>ADASYN</strong> to balance the dataset effectively and improve the detection of fraudulent claims. Additionally, exploring more robust 
       ensemble methods such as <strong>XGBoost</strong>, <strong>CatBoost</strong>, and <strong>LightGBM</strong> can further enhance model performance, 
       especially in handling imbalanced data. Integrating anomaly detection techniques like <strong>Isolation Forest</strong> and <strong>One-Class SVM</strong> 
       can also be beneficial, as they are particularly effective in identifying rare events like fraud.</p>

    <p>Moreover, to make the fraud detection system more reliable and practical, future efforts will focus on deploying the model in real-time scenarios. 
       Implementing a robust API and integrating the model into existing insurance processing systems will enable live fraud scoring. Continuous model monitoring 
       and threshold tuning will be essential to maintain accuracy over time. Additionally, enriching the dataset by incorporating external metadata, such as credit 
       history and telematics data, can provide deeper insights and improve model accuracy. By focusing on these areas, the developed system will become more scalable 
       and effective in combating insurance fraud in real-world applications.</p>
</div>
        </section>

        <!-- Conclusion -->
        <section id="conclusion">

<div class="section">
    <h1>12. Conclusion</h1>
    <p>This project successfully demonstrated the use of metadata-driven machine learning techniques to detect fraudulent vehicle insurance claims. 
       By leveraging key features such as address change frequency, policy tenure, and claim filing delay, the developed models effectively identified suspicious claims. 
       Among the models tested, the <strong>Gradient Boosting</strong> model showed the highest performance, achieving an <strong>AUC of 0.8059</strong>. Despite the 
       high accuracy, the model faced challenges related to class imbalance, which affected its recall for detecting fraudulent cases. The project also highlighted 
       the importance of feature engineering and model selection to enhance predictive performance. Addressing the trade-off between precision and recall proved 
       to be a critical aspect in optimizing the model.</p>

    <p>         While the current model shows promising results, there are areas that need further improvement to ensure reliable real-world implementation. 
       Addressing class imbalance remains a top priority, as it significantly impacts the model's ability to detect fraud effectively. Implementing advanced techniques, 
       optimizing model parameters, and incorporating richer metadata will be essential for building a more robust and scalable system. Real-time deployment and 
       continuous monitoring will also play a crucial role in maintaining model accuracy and adaptability. By implementing these improvements, the developed fraud 
       detection system can become a valuable tool in the insurance industry, helping to reduce financial losses and enhance the efficiency of claims processing.</p>
</div>
        </section>

<!-- Team Members Section -->
<section id="team" style="position: fixed; bottom: 10px; right: 10px; background-color: #f4f4f4; padding: 10px; border-radius: 5px; box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);">
    <div class="section" style="text-align: right;">
        <h1 style="font-size: 1.2em; color: #2c3e50; margin-bottom: 5px;">Team Members</h1>
        <div class="team-list" style="font-size: 0.85em; color: #3498db;">
            <p>Yatish Chandra Nalla (G01512626)</p>
            <p>Sai Vamsidhar Reddy Meruva (G01474023)</p>
            <p>Manasa Rajagopal Madabushi (G01476994)</p>
            <p>Sai Raga Sruthi Nalagandla (G01478476)</p>
            <p>Sonal Ashok Kumar Masid (G01514651)</p>
            <p>Madhu Yamini Nainala (G01483862)</p>
            <p>Sowdhamini Bharadwaj Mandava Venkata (G01476219)</p>
        </div>
    </div>
</section>
</main>

<footer>
    <div class="footer-content">
        <p>&copy; 2025 AIT 582 Team 4 - George Mason University</p>
        <img src="assets/gmu_logo.png" alt="George Mason University" width="60" height="60">
    </div>
</footer>
</body>
</html>